%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required

%
\RequirePackage{fix-cm}
%
\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{moreverb,url}
\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{lineno}
\usepackage[misc]{ifsym}
\usepackage{bm}
\usepackage{amsmath}  
\usepackage{float}
\usepackage{multirow}
\usepackage{threeparttable}  
\usepackage{booktabs}
\usepackage{color}
\usepackage{amssymb}
\definecolor{revision_switch}{RGB}{255,0,0}
\graphicspath{{./fig/}}







%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
\journalname{Journal of Geodesy}
%
\begin{document}
\linenumbers
\title{GNSS displacement detection based on Bayesian inference
%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}


%\titlerunning{Short form of title}        % if too long for running head

\author{Nan Shen\textsuperscript{1}\and
		Liang Chen\textsuperscript{12*}\and        
		Ruizhi Chen\textsuperscript{12} %etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{
	\Letter Liang Chen\\
	{l.chen@whu.edu.cn}\\       
	\at
    {1} State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, 129 Luoyu Road, Wuhan 430079, China
    \at
    {2} Collaborative Innovation Center for Geospatial Technology,  129 Luoyu Road, Wuhan 430079, China
    \\
}

\date{Received: date }
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}

\keywords{GNSS\and unmodeled error\and machine learning\and convolutional neural network\and wavelet coherence}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}



\section{Methodology}
\subsection{Relative real-time kinematic (RTK)}
RTK is widely used in structural health monitoring, landslide monitoring, and other fields due to its high positioning accuracy and real-time efficiency. The observation model, the state model, and the extended Kalman filtering process of RTK are introduced below. 
\subsubsection{Observation model}
The double-difference (DD) observation equation of relative positioning among base receiver, rover receiver, and satellites $j$, $k$ is defined in (\ref{eq_dd_obs})\citep{teunissen2017springer}.
\begin{figure*}
	\begin{equation}
	\left. \begin{array}{l}
	\Delta \nabla P_{{i_{br}}}^{jk} = \Delta \nabla \rho _{br}^{jk} - \Delta \nabla \delta I_{\Phi {i_{br}}}^{jk} + \Delta \nabla \delta _{tro{p_{br}}}^{jk} + \Delta \nabla \delta M_{{P_{{i_{br}}}}}^{jk} + \Delta \nabla \varepsilon _{{P_{{i_{br}}}}}^{jk}\\
	\Delta \nabla \Phi _{{i_{br}}}^{jk} = \Delta \nabla \rho _{br}^{jk} + \Delta \nabla \delta I_{{\Phi _{{i_{br}}}}}^{jk} + \Delta \nabla \delta _{tro{p_{br}}}^{jk} - {\lambda _i}\Delta \nabla N_{{i_{br}}}^{jk} + \Delta \nabla \delta M_{{\Phi _{{i_{br}}}}}^{jk} + \Delta \nabla \varepsilon _{{\Phi _{{i_{br}}}}}^{jk}
	\end{array} \right\}
	\label{eq_dd_obs}
	\end{equation}
\end{figure*}
In this observation equation, $\Delta \nabla $ denotes the DD operator, $i$ denotes the frequency; $b$ and $r$ denote base station and rover station respectively; $\Delta \nabla P_{{{i}_{br}}}^{jk}$ and $\Delta \nabla \Phi _{{{i}_{br}}}^{jk}$ denote pseudo-range and carrier-phase DD observations, respectively; $\Delta \nabla \rho _{br}^{jk}$ is the DD geometric distance between receivers and satellites; $\Delta \nabla \delta M_{{{P}_{{{i}_{br}}}}}^{jk}$ and $\Delta \nabla \delta M_{{{\Phi }_{{{i}_{br}}}}}^{jk}$ denote pseudo-range and carrier-phase multipath error, respectively; $\Delta \nabla \varepsilon _{{{P}_{{{i}_{br}}}}}^{jk}$ and $\Delta \nabla \varepsilon _{{{\Phi }_{{{i}_{br}}}}}^{jk}$ represent DD observation error of pseudo-range and carrier-phase, respectively; $\Delta \nabla \delta I_{\Phi {{i}_{br}}}^{jk}$is the DD ionospheric delay between receivers and satellites, and the ionospheric error has the same absolute value but the opposite sign on pseudo-range and carrier-phase observations; $\Delta \nabla \delta _{tro{{p}_{br}}}^{jk}$is the DD tropospheric delay between receivers and satellites. For the short baseline, the atmospheric error can be ignored, so the above DD observation equation is simplified as (\ref{eq_dd_obs_simplified}).
\begin{figure*}
	\begin{equation}
	\left. \begin{array}{l}
	\Delta \nabla P_{{i_{br}}}^{jk} = \Delta \nabla \rho _{br}^{jk} + \Delta \nabla \delta M_{{P_{{i_{br}}}}}^{jk} + \Delta \nabla \varepsilon _{{P_{{i_{br}}}}}^{jk}\\
	\Delta \nabla \Phi _{{i_{br}}}^{jk} = \Delta \nabla \rho _{br}^{jk} - {\lambda _i}\Delta \nabla N_{{i_{br}}}^{jk} + \Delta \nabla \delta M_{{\Phi _{{i_{br}}}}}^{jk} + \Delta \nabla \varepsilon _{{\Phi _{{i_{br}}}}}^{jk}\end{array} \right\}
	\label{eq_dd_obs_simplified}
	\end{equation}
	
\end{figure*}
By ignoring the influence of multipath, linearizing equation (\ref{eq_dd_obs_simplified}), we get
\begin{equation}
\label{eq_rtk_measure}
{\bf{y}} = {\bf{Hx}} + {\bf{\varepsilon }}
\end{equation}
where $\bf{y}$ is the observation minus the computed,  $\bf{H} $ is the linearized design matrix\citep{hofmann-wellenhof2007gnss}, $\bf{x}\triangleq {{[{{\mathbf{r}}_{r}},\Delta \nabla \mathbf{N}]}^{T}}$, is the state vector, which contains the rover station coordinate ${{\mathbf{r}}_{r}}$ and the DD integer ambiguity $\Delta \nabla \mathbf{N}$;  $\mathbf{\varepsilon } $ is the observation noise, and its covariance matrix is expressed as  ${{\mathbf{R}}_{\varepsilon }} $. After forming the observation equation, the ‘float’ solution of the parameter can be obtained by the standard least square method. To obtain high precision positioning results, it is necessary to fix the integer ambiguity. The 'float' solution and variance of integer ambiguity are expressed as  $\Delta \nabla \mathbf{\tilde{N}} $,  ${{\mathbf{Q}}_{\Delta \nabla \mathbf{\tilde{N}}}} $, respectively. Then the problem of ambiguity fixing can be described as (\ref{eq_lamda})\citep{teunissen1995the}.
\begin{figure*}
	\begin{equation}
	\Delta \nabla {\bf{\hat N}} = \mathop {\arg min}\limits_{\Delta \nabla {\bf{N}} \in {\bf{Z}}} ((\Delta \nabla {\bf{\tilde N}} - \Delta \nabla {\bf{\hat N}}){\bf{Q}}_{\Delta \nabla {\bf{\tilde N}}}^{ - 1}{(\Delta \nabla {\bf{\tilde N}} - \Delta \nabla {\bf{\hat N}})^T})
	\label{eq_lamda}
	\end{equation}
\end{figure*}
There are many methods referring to integer ambiguity resolution, among which LAMBDA (Least-squares Ambiguity Decorrelation Adjustment)\citep{teunissen1995the} method is the most widely used one. The integer ambiguity is obtained by the integer least square method. Then, the test process is executed to determine whether to accept or reject the integer ambiguity\citep{wang2014ambiguity,wang2018improving}. If the test is passed, the fixed integer ambiguity can be substituted into (\ref{eq_rtk_measure}) to resolve the fixed solution, otherwise, the ‘float’ solution will be maintained.

\subsubsection{State model}
The state model is defined as follows (Takasu 2013)
\begin{equation}
{{\bf{x}}_k} = {\bf{F}}{{\bf{x}}_{k - 1}} + {\bf{w}}
\end{equation}
where ${\bf{F}} \buildrel \Delta \over = \left[ {\begin{array}{*{20}{c}}
	{{{\bf{I}}_{3 \times 3}}}&{\bf{0}}\\
	{\bf{0}}&{{{\bf{I}}_{(2n - 2)(2n - 2)}}}
	\end{array}} \right]$ , is the transition matrix from $k-1$ to $k$; $\mathbf{I}$ denotes the identity matrix; $n$ represents the number of satellites observed simultaneously by the base station and the rover station; Only GPS ${{L}_{1}}$ and ${{L}_{2}}$ frequency observations are considered, so $2n-2$ DD ambiguities are formed. $\mathbf{w}$ is the process noise, and its covariance matrix is expressed as ${{\mathbf{G}}_{w}}$. For kinematic positioning, the coordinate covariance component is set to be large to capture dynamic features, while the ambiguity covariance component is set to $\mathbf{0}$ to ensure the stability of positioning.
\subsubsection{Extended Kalman filtering}
Kalman filtering includes two processes: prediction update and measurement update\citep{grewal2001kalman}. The prediction process is as follows:
\begin{equation}
{{\bf{\hat x}}_{k|k - 1}} = {\bf{F}}{{\bf{\hat x}}_{k - 1}}
\end{equation}
\begin{equation}
{{\bf{Q}}_{k|k - 1}} = {\bf{F}}{{\bf{Q}}_{k - 1}}{{\bf{F}}^T} + {{\bf{G}}_w}(k)
\end{equation}
According to the definition of the state transition matrix above, it can be seen that the state value of the previous epoch remains unchanged. However, the coordinate covariance component becomes larger, while the ambiguity covariance component remains unchanged. The measurement update process is as follows:
\begin{equation}
{{\bf{v}}_k} = {\bf{y}}(k) - {\bf{H}}(k){{\bf{\hat x}}_{k|k - 1}}
\end{equation}
\begin{equation}
{{\bf{S}}_k} = {\bf{H}}(k){{\bf{Q}}_{k|k - 1}}{\bf{H}}{(k)^{\rm{T}}} + {{\bf{R}}_\varepsilon }(k)
\end{equation}
\begin{equation}
{\bf{K}} = {{\bf{Q}}_{k|k - 1}}{\bf{H}}_k^T{\bf{S}}_k^{ - 1}
\end{equation}
where ${{\mathbf{v}}_{k}}$ denotes the innovation vector, and ${{\mathbf{S}}_{k}}$ is the covariance matrix of the innovation vector; $\mathbf{K}$ is the state estimation gain matrix. The final state is estimated as follows:
\begin{equation}
{{\bf{\hat x}}_k} = {{\bf{\hat x}}_{k|k - 1}} + {\bf{K}}{{\bf{v}}_k}
\end{equation}
\begin{equation}
{{\bf{Q}}_k} = {{\bf{Q}}_{k|k - 1}} - {\bf{K}}{{\bf{S}}_k}{{\bf{K}}^T}
\end{equation}
where ${{\mathbf{\hat{x}}}_{k}}$ is the final estimate, and ${{\mathbf{Q}}_{k}}$ is the corresponding covariance matrix; ${{\mathbf{\hat{x}}}_{k}}$ is the only input of the proposed method.  

Due to the complex spatiotemporal characteristics of observation errors, there are unmodeled error components in time series\citep{shen2020site}. To weaken the influence of these factors, the wavelet analysis tool is introduced as follows.




\subsection{Displacement Detection}
The displacement time series is described as $\left[ {{x_1},{x_2}, \cdots {x_m}} \right]$, where $m$ is the number of coordinate displacements. The time of displacement changes is expressed as $\left[ {{\tau _1},{\tau _2}, \cdots {\tau _n}} \right]$, where $n$ is the number of change points. The problem of displacement detection is defined as finding out the change points of displacement and giving the effective estimation of displacement.

\subsubsection{Bayesian Inference}
Bayesian inference is one of the most important skills in statistics. Bayesian inference deduces the posterior probability as the result of a priori probability and likelihood function. Bayesian inference calculates the posterior probability according to the Bayesian theorem

\begin{equation}\label{eq_bayesian_inference}
P({\bf{\theta }}\left| {\bf{x}} \right.) = \frac{{P({\bf{\theta }})P({\bf{x}}\left| {\bf{\theta }} \right.)}}{{P({\bf{x}})}}
\end{equation}
where $\bf{\theta }$ is the parameter to be estimated and $\bf{x}$ is the observation;$P({\bf{\theta }}\left| {\bf{x}} \right.)$ denotes the posterior probability; $P({\bf{\theta }})$ represents a priori probability, which refers to the probability obtained from previous experience and analysis;
$P({\bf{x}}\left| {\bf{\theta }} \right.)$ is the likelihood function, which represents the probability of $x$ when a priori is established. ${P({\bf{x}})}$ is the total likelihood, which is a constant value.

\subsubsection{Likelihood Function}
It is assumed that the displacement obtained by GNSS obeys Gaussian normal distribution. The displacement of each segment segmented by the change point obeys the Gaussian distribution of different mean and same variance, which is expressed as follows

\begin{equation}\label{eq_ts_cps}
x \sim \left\{ {\begin{array}{*{20}{r}}
	{N({\mu _0},\sigma ),}&{t < {\tau _1}}\\
	{N({\mu _1},\sigma ),}&{{\tau _1} \le t < {\tau _2}}\\
	\vdots &{}\\
	{\begin{array}{*{20}{c}}
		{N({\mu _n},\sigma ),}
		\end{array}}&{{\tau _{n - 1}} \le t < {\tau _n}}\\
	{N({\mu _{n + 1}},\sigma ),}&{{\tau _n} \le t}
	\end{array}} \right.
\end{equation}
where the $n$ change points divide the time series into $n+1$ segments, and the mean values of each segment are ${\mu _0},{\mu _1},\cdots,{\mu _n}$; $\sigma$ is the standard deviation of each normal distribution; Then the likelihood probability is expressed as follows

\begin{equation}\label{eq_likelihood}
P({\bf{x}}\left| {\bf{\theta }} \right.) = P({\bf{x}}\left| {{\mu_0},{\mu_1},} \right. \cdots {\mu_n},{\tau _1},{\tau _2}, \cdots ,{\tau _n},\sigma )
\end{equation}
where ${\bf{\theta }}=({{\mu_0},{\mu_1},} \cdots {\mu_n},{\tau _1},{\tau _2}, \cdots ,{\tau _n},\sigma) $; Assuming that the displacement observations are independent of each other, the likelihood probability can be expressed as

\begin{equation}\label{eq_likelihoodfunc}
P({\bf{x}}\left| {\bf{\theta }} \right.) = \prod\limits_{x \le {\tau _1}} {P(x\left| {{\mu_0},\sigma } \right.)} \prod\limits_{i = 1}^{n - 1} {\prod\limits_{{\tau _i} \le x < {\tau _{i + 1}}} {P(x\left| {{\mu_i},\sigma } \right.)} \prod\limits_{{\tau _n} \le x} {P(x\left| {{\mu_n},\sigma } \right.)} }.
\end{equation}
Combine \ref{eq_ts_cps} and \ref{eq_likelihoodfunc} to get the likelihood function as follows

\begin{equation}\label{eq_likelihoodfunc_detail}
P({\bf{x}}\left| {\bf{\theta }} \right.) = \frac{1}{{{{(2\pi {\sigma ^2})}^{m/2}}}}\exp \left\{ {\frac{1}{{ - 2{\sigma ^2}}}\left[ {\sum\limits_{x < {\tau _1}} {{{(x - {\mu_0})}^2}} {\rm{ + }}\sum\limits_{i = 1}^{n - 1} {\sum\limits_{{\tau _i} \le x < {\tau _{i + 1}}} {{{(x - {\mu_i})}^2}} } {\rm{ + }}\sum\limits_{{\tau _n} \le x} {{{(x - {\mu_n})}^2}} } \right]} \right\}
\end{equation}

\subsubsection{Priors}
\begin{equation}\label{eq_bayesian_prior}
P({\bf{\theta }}) = P({\bf{\tau }},{\bf{\mu }},\sigma ) = P({\bf{\tau }})P({\bf{\mu }})P(\sigma )
\end{equation}
\begin{equation}\label{eq_bayesian_prior_sigma}
\sigma  \sim N({\mu _\sigma },{\sigma _\sigma })
\end{equation}
\begin{equation}\label{eq_bayesian_prior_mu}
{\mu _i} \sim N({\mu _\mu },{\sigma _\mu })(i = 0,1, \cdots ,n)
\end{equation}
\begin{equation}\label{eq_bayesian_prior_mu_sum}
P({\bf{\mu }}) = \prod\limits_{i = 0}^n {P({\mu _i})}
\end{equation}


\begin{equation}\label{eq_bayesian_prior_tau_basic}
P({\tau _i}) = \left\{ {\begin{array}{*{20}{r}}
	{{1 \mathord{\left/
				{\vphantom {1 {({t_m} - {t_1}),}}} \right.
				\kern-\nulldelimiterspace} {({t_m} - {t_1}),}}}&{i = 1}\\
	{{1 \mathord{\left/
				{\vphantom {1 {({t_m} - {\tau _{i - 1}}),}}} \right.
				\kern-\nulldelimiterspace} {({t_m} - {\tau _{i - 1}}),}}}&{1 < i \le n}
	\end{array}} \right.
\end{equation}
\begin{equation}\label{eq_bayesian_prior_tau_1}
P({\bf{\tau }}) = P({\tau _1},{\tau _2}, \cdots ,{\tau _{n - 1}},{\tau _n})
\end{equation}
\begin{equation}\label{eq_bayesian_prior_tau_2}
P({\bf{\tau }}) = P({\tau _n},{\tau _{n - 1}}, \cdots ,{\tau _2}\left| {{\tau _1}} \right.)P({\tau _1})
\end{equation}
\begin{equation}\label{eq_bayesian_prior_tau_3}
P({\bf{\tau }}) = P({\tau _n},{\tau _{n - 1}}, \cdots ,{\tau _3}\left| {{\tau _2},{\tau _1}} \right.)P(\left. {{\tau _2}} \right|{\tau _1})P({\tau _1})
\end{equation}
\begin{equation}\label{eq_bayesian_prior_tau_4}
P({\bf{\tau }}) = P({\tau _n},{\tau _{n - 1}}, \cdots ,{\tau _3}\left| {{\tau _2}} \right.)P(\left. {{\tau _2}} \right|{\tau _1})P({\tau _1})
\end{equation}
\begin{equation}\label{eq_bayesian_prior_tau_5}
P({\bf{\tau }}) = P({\tau _n}\left| {{\tau _{n - 1}}} \right.) \cdots P({\tau _3}\left| {{\tau _2}} \right.)P(\left. {{\tau _2}} \right|{\tau _1})P({\tau _1})
\end{equation}

\subsubsection{Markov Chain Monte Carlo (MCMC)}
The posterior distribution of the parameters depends on the prior distribution and the likelihood function. Obtaining the posterior probability of parameters by direct integration or sum requires a lot of operations, which is difficult to achieve. In this work, the Markov chain Monte Carlo technique is used to approximate the integral or sum value. Monte Carlo sampling is to construct the integral or sum function as the expectation under a certain distribution, and then approximate the expectation by the corresponding average value\citep{goodfellow2016deep}.

\begin{equation}\label{eq_monte_carlo_1}
s =\sum\limits_x g(x)= \sum\limits_x {p(x)f(x) = {E_p}[f(x)]}
\end{equation}
\begin{equation}\label{eq_monte_carlo_2}
s =\int {g(x)}= \int {p(x)f(x)dx}  = {E_p}[f(x)]
\end{equation}
where $g(x)$ is the function to be integrated or summated; $p(x)$ is the probability distribution or probability density function of $x$. The integral or summation problem of $g(x)$ is transformed into the mathematical expectation problem of $f(x)$ under the distribution $p(x)$. This mathematical expectation can be approximated by sampling from $p(x)$.

\begin{equation}\label{eq_monte_carlo_3}
\hat s = \frac{1}{n}\sum\limits_{i = 1}^n {f({x_i})}
\end{equation}
where $x_i$ is the sample with the distribution of $p(x)$. As can be seen from the above, Monte Carlo method as a general sampling simulation sum or integration method, depends on sampling from the distribution of $p(x)$. However, it is not easy to directly sample from the target distribution of $p(x)$. At present, the most popular solution is to use Markov chain Monte Carlo sampling, which provides a general way to construct a sequence that converges to the target distribution. According to Markov chain, the probability of state transition at a certain time only depends on its previous state, which is expressed as follows

\begin{equation}\label{eq_markov_chain_1}
P({x_{n + 1}}\left| {{x_n},{x_{n - 1}}} \right.,{x_{n - 2}}, \cdots ,{x_2},{x_1}) = P({x_{n + 1}}\left| {{x_n}} \right.)
\end{equation}
If a Markov process is neither periodic nor irreducible, then the Markov process is ergodic, that is, each state appears with a certain probability. The Markov process of ergodic states has an important property: no matter what the initial state probability is, the Markov process of the ergodic state tends to be a stable distribution after enough state transitions.
That is, for the initial probability distribution is $\pi_0(x)$, the probability distribution after several rounds of state transition is as follows

\begin{equation}\label{eq_markov_chain_2}
\pi_{0}(x),\pi_{1}(x),\pi_{2}(x),\cdots \pi_{n}(x),\pi_{n+1}(x),\pi_{n+2}(x)\cdots
\end{equation}
There exists $n$ which satisfies the following conditions

\begin{equation}\label{eq_markov_chain_3}
\pi_{n}(x)=\pi_{n+1}(x)=\pi_{n+2}(x)\cdots
\end{equation}
This is the target distribution we want to construct. Based on the initial arbitrary simple probability distribution such as Gaussian distribution $\pi_{0}(x)$, the state value $x_0$ is obtained by sampling. Based on the probability distribution $\pi_{1}(x)=\pi_{0}(x)P(x\left| {{x_0}} \right.)$ sampling state value $x_1$, using the same method, execute $n$ times. Sample $(x_{n},x_{n+1},x_{n+2}\cdots)$ is the corresponding sample set of the target distribution. However, the implementation of MCMC also needs to consider other factors such as constructing the state transition matrix according to the target distribution, which is beyond the scope of this work. Two implementations of MCMC,  the Metropolis-Hasting sampler\citep{chib1995understanding} and the No-U-Turn sampler\citep{hoffman2014no}, are used to sample discrete random variables and continuous random variables, respectively.


\subsection{Displacement Extraction}



\section{Experiments and Results}
To verify the feasibility of the proposed method, we carried out a series of experiments, including a simulation experiment and a field experiment. In the simulation experiment, data affected by noise and a trend item is simulated. In the field experiment, several displacement changes were triggered manually, and the relative real-time kinematic model is used to process the GNSS observations.
\subsection{Simulation experiment}


\subsection{Field experiment}
The experiment was carried out on the roof of a building on the campus of Wuhan University. In the experiment, the equipment deployment is shown in Fig. \ref{fig_field_exp_config}. A hard plank is pressed with large stones to ensure that the plank remains as fixed as possible during moving the object hanging on the plank. One receiver antenna is fixed on the plank as a rover; the other receiver antenna is fixed on a tripod beside it as the base station. Both antennas are connected to BD992 OEM boards on the table, and the OEM boards are connected to the laptops for data collection. As shown in subgraphs (c) and (d), the height of the antenna is measured with a tape before and after each movement of the object hanging on the plank. Besides, markers are made on the ground and on the antenna respectively to ensure that the same position is referenced for each measurement. The sampling frequency of the GNSS receiver is set to 1 Hz. The whole experiment lasted for nearly one hour, during which the object hanging on the plank was moved several times manually.

\section{Discussion}


\section{Conclusions}


\begin{acknowledgements}
This research was funded by the National Key Research and Development Programs 2018YFB0505400, the Natural Science Fund of Hubei Province 2018CFA007.
\end{acknowledgements}

%
\section*{Conflict of Interest}
The authors declare that they have no conflict of interest.
\section*{Author Contributions}
Conceptualization: Nan Shen; Methodology: Nan Shen; Writing - original draft preparation: Nan Shen; Writing - review and editing:Ruizhi Chen; Supervision: Liang Chen.
 \section*{Data Availability}


% BibTeX users please use one of
\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{ref}


\end{document}


